{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tpot import TPOTClassifier\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_path = 'train'\n",
        "predictors_paths = {\n",
        "    'GM': os.path.join(train_data_path, 'GM.csv'),\n",
        "    'WM': os.path.join(train_data_path, 'WM.csv'),\n",
        "    'DMN': os.path.join(train_data_path, 'DMN.csv'),\n",
        "    'FA': os.path.join(train_data_path, 'FA.csv'),\n",
        "    'MD': os.path.join(train_data_path, 'MD.csv')\n",
        "}\n",
        "additional_variables_path = os.path.join(train_data_path, 'Subjects.csv')\n",
        "\n",
        "# Predictors\n",
        "modalities = ['GM', 'DMN', 'FA']\n",
        "selected_modalities = {modality: predictors_paths[modality] for modality in modalities if modality in predictors_paths}\n",
        "def read_and_rename(modality, path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={label: f\"{label}_{modality}\" for label in df.columns if label != 'ID'})\n",
        "    return df\n",
        "dfs = [read_and_rename(modality, path) for modality, path in selected_modalities.items()]\n",
        "predictors_df = reduce(lambda left, right: pd.merge(left, right, on='ID'), dfs)\n",
        "\n",
        "# Response and confounding variables\n",
        "additional_variables = ['Sex', 'Age']\n",
        "df = pd.read_csv(additional_variables_path)\n",
        "selected_variables = [variable for variable in additional_variables if variable in df.columns]\n",
        "additional_variables_df = df[[\"ID\"] + selected_variables]\n",
        "\n",
        "# Merge predictors with response and confounding variables on 'ID'\n",
        "df = pd.merge(additional_variables_df, predictors_df, on='ID')\n",
        "\n",
        "# Prepare X and y\n",
        "X = df.drop(columns=['ID', 'Sex'])\n",
        "y = df['Sex']\n",
        "\n",
        "# Split into training and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f'Sample size for training: {X_train.shape[0]}')\n",
        "print(f'Sample size for test: {X_test.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AutoML: TPOT (Tree-based Pipeline Optimization Tool)\n",
        "$ \\text{Total pipelines evaluated} = \\text{population size} + (\\text{generations} \\times \\text{offspring size}) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# http://epistasislab.github.io/tpot\n",
        "# generations: number of iterations to run the pipeline optimization process\n",
        "# population_size: number of individuals to retain in the genetic programming (GP) population every generation\n",
        "# offspring_size: number of offspring to produce in each GP generation; by default, offspring_size = population_size\n",
        "# total pipelines evaluated = 50 + (5 x 50) = 300\n",
        "tpot = TPOTClassifier(\n",
        "    generations=5, \n",
        "    population_size=50, \n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    max_time_mins=None,\n",
        "    scorers=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tpot.fit(X_train, y_train)\n",
        "predictions = tpot.predict(X_test)\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy by TPOT: {acc:.3f}\")\n",
        "best_pipeline = tpot.fitted_pipeline_\n",
        "print(\"Best pipeline steps:\")\n",
        "for step in best_pipeline.steps:\n",
        "    print(step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = best_pipeline[:-1]\n",
        "classifier = best_pipeline.steps[-1][1]\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "\n",
        "# Features\n",
        "feature_names = None\n",
        "try:\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "except (AttributeError, Exception) as e:\n",
        "    if isinstance(X_train, pd.DataFrame) and X_train_transformed.shape[1] == len(X_train.columns):\n",
        "        feature_names = X_train.columns\n",
        "    else:\n",
        "        feature_names = [f\"feature_{i}\" for i in range(X_train_transformed.shape[1])]\n",
        "\n",
        "# Importances\n",
        "if hasattr(classifier, 'feature_importances_'):\n",
        "    print(f\"{classifier.__class__.__name__} has feature_importances_ attribute.\")\n",
        "    feature_importances = classifier.feature_importances_\n",
        "elif hasattr(classifier, 'coef_'):\n",
        "    print(f\"{classifier.__class__.__name__} has coef_ attribute.\")\n",
        "    feature_importances = classifier.coef_\n",
        "    if feature_importances.ndim > 1:\n",
        "        feature_importances = feature_importances[0]\n",
        "else:\n",
        "    print(f\"{classifier.__class__.__name__} does not have feature_importances_ or coef_ attribute.\")\n",
        "    feature_importances = None\n",
        "\n",
        "# Print features and their importances\n",
        "if feature_importances is not None:\n",
        "    features_and_importances = zip(feature_names, feature_importances)\n",
        "    sorted_features_and_importances = sorted(features_and_importances, key=lambda x: x[1], reverse=True)\n",
        "    top_features_and_importances = sorted_features_and_importances[:5]\n",
        "    print(f\"Top features' importances by TPOT:\")\n",
        "    for no, feature_importance in enumerate(top_features_and_importances):\n",
        "        print(f\"{no + 1}. {feature_importance[0]}: {feature_importance[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAP (SHapley Additive exPlanations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_shap_values(preprocessor, model, X_test, n_samples_background=100):\n",
        "    X_test_transformed = preprocessor.transform(X_test)\n",
        "    is_classifier = hasattr(model, 'predict_proba')\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_test_transformed)\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            explainer = shap.LinearExplainer(model, X_test_transformed)\n",
        "            shap_values = explainer.shap_values(X_test_transformed)\n",
        "        else:\n",
        "            X_background = shap.sample(X_test_transformed, n_samples_background)\n",
        "            if is_classifier:\n",
        "                predict_function = model.predict_proba\n",
        "            else:\n",
        "                predict_function = model.predict\n",
        "            explainer = shap.KernelExplainer(predict_function, X_background)\n",
        "            shap_values = explainer.shap_values(X_test_transformed)\n",
        "    except Exception as e:\n",
        "        print(f\"First explainer failed: {e}. Falling back to KernelExplainer.\")\n",
        "        X_background = shap.sample(X_test_transformed, n_samples_background)\n",
        "        if is_classifier:\n",
        "            predict_function = model.predict_proba\n",
        "        else:\n",
        "            predict_function = model.predict\n",
        "        explainer = shap.KernelExplainer(predict_function, X_background)\n",
        "        shap_values = explainer.shap_values(X_test_transformed)\n",
        "    return explainer, shap_values, X_test_transformed\n",
        "\n",
        "# Compute SHAP values\n",
        "explainer, shap_values, X_test_transformed = get_shap_values(preprocessor, classifier, X_test)\n",
        "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, max_display=5)\n",
        "\n",
        "# Correlate between original and transformed features\n",
        "class_idx = 1\n",
        "class_shap_values = shap_values[:, :, class_idx]\n",
        "mean_abs_shap = np.abs(class_shap_values).mean(axis=0)\n",
        "feature_importance_indices = np.argsort(-mean_abs_shap)\n",
        "if hasattr(X_test, 'columns'):\n",
        "    original_features = X_test.columns\n",
        "    for idx in feature_importance_indices[:5]:\n",
        "        feature_values = X_test_transformed[:, idx]\n",
        "        correlations = {}\n",
        "        for col in original_features:\n",
        "            correlations[col] = np.corrcoef(feature_values, X_test[col])[0, 1]\n",
        "        sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "        print(f\"Original features most correlated with transformed feature {idx}:\")\n",
        "        for feat, corr in sorted_correlations[:3]:\n",
        "            print(f\"  - {feat}: {corr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data_path = 'test'\n",
        "predictors_paths = {\n",
        "    'GM': os.path.join(test_data_path, 'GM.csv'),\n",
        "    'WM': os.path.join(test_data_path, 'WM.csv'),\n",
        "    'DMN': os.path.join(test_data_path, 'DMN.csv'),\n",
        "    'FA': os.path.join(test_data_path, 'FA.csv'),\n",
        "    'MD': os.path.join(test_data_path, 'MD.csv')\n",
        "}\n",
        "additional_variables_path = os.path.join(test_data_path, 'Subjects.csv')\n",
        "\n",
        "# Predictors\n",
        "selected_modalities = {modality: predictors_paths[modality] for modality in modalities if modality in predictors_paths}\n",
        "dfs = [read_and_rename(modality, path) for modality, path in selected_modalities.items()]\n",
        "predictors_df = reduce(lambda left, right: pd.merge(left, right, on='ID'), dfs)\n",
        "\n",
        "# Confounding variables\n",
        "df = pd.read_csv(additional_variables_path)\n",
        "selected_variables = [variable for variable in additional_variables if variable in df.columns]\n",
        "additional_variables_df = df[['ID'] + selected_variables]\n",
        "\n",
        "# Merge predictors with confounding variables on 'ID'\n",
        "df = pd.merge(additional_variables_df, predictors_df, on='ID')\n",
        "\n",
        "# Apply trained model\n",
        "# Apply trained model\n",
        "X_ext = df.drop(columns=['ID'])\n",
        "X_ext_transformed = preprocessor.transform(X_ext)\n",
        "predictions_ext = classifier.predict(X_ext_transformed)\n",
        "\n",
        "# Save predictions\n",
        "np.savetxt(os.path.join(test_data_path, \"Predictions.txt\"), predictions_ext)\n",
        "\n",
        "# Compute SHAP values\n",
        "shap_values = explainer.shap_values(X_ext_transformed)\n",
        "shap.summary_plot(shap_values, X_ext_transformed, feature_names=feature_names, max_display=5)\n",
        "\n",
        "# Correlate between original and transformed features\n",
        "class_idx = 1\n",
        "class_shap_values = shap_values[:, :, class_idx]\n",
        "mean_abs_shap = np.abs(class_shap_values).mean(axis=0)\n",
        "feature_importance_indices = np.argsort(-mean_abs_shap)\n",
        "if hasattr(X_ext, 'columns'):\n",
        "    original_features = X_ext.columns\n",
        "    for idx in feature_importance_indices[:5]:\n",
        "        feature_values = X_ext_transformed[:, idx]\n",
        "        correlations = {}\n",
        "        for col in original_features:\n",
        "            correlations[col] = np.corrcoef(feature_values, X_ext[col])[0, 1]\n",
        "        sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "        print(f\"Original features most correlated with transformed feature {idx}:\")\n",
        "        for feat, corr in sorted_correlations[:3]:\n",
        "            print(f\"  - {feat}: {corr:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Anonymous",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}